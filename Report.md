# Spotify Data Pipeline â€“ Technical Report

## 1. Project Introduction

This project implements an **end-to-end, containerized data pipeline** for ingesting Spotify track data and transforming it into analytics-ready models.  
A key design principle of this pipeline is **idempotency**: each run can be executed multiple times without producing duplicate or inconsistent results. This ensures reliability, easier recovery from failures, and safe re-runs during development or production operations.

The pipeline is fully orchestrated by **Apache Airflow**, with all services running inside Docker containers.

---

## 2. Design and Architecture Choices

### High-level Architecture

![Data Architecture Diagram](images/data_architecture_diagram.jpg)
### Components and Their Roles

- **Spotify API**  
  Source of raw JSON track data.

- **Apache Airflow**  
  Orchestrates the entire pipeline end to end: ingestion, validation, transformation, and loading.  
  Airflow enables retries, scheduling, monitoring, and dependency management between tasks.

- **MinIO (S3-compatible Object Storage)**  
  Stores raw ingested JSON data and intermediate artifacts.  
  Acts as a durable, scalable, and decoupled storage layer between ingestion and transformation.

- **PostgreSQL (Staging & Warehouse)**  
  Hosts staging tables and downstream analytical models generated by dbt.

- **dbt (Data Build Tool)**  
  Transforms staged data into clean, analytics-ready models and enforces data quality through tests.

### Data Flow

1. Airflow triggers ingestion tasks.
2. Raw JSON data is fetched from the Spotify API.
3. Data is validated to ensure schema and format correctness.
4. Validated raw data is written to MinIO as JSON objects.
5. Transformation logic prepares data for the staging environment.
6. Transformed data is loaded into PostgreSQL staging tables.
7. dbt models transform staging data into analytical models.
8. dbt tests validate both staging and reporting layers.

---

## 3. Optimization Strategies Applied

### Object Storage with MinIO
- MinIO provides **S3-compatible, high-throughput object storage**.
- Decouples ingestion from downstream processing.
- Enables efficient handling of large JSON payloads.
- Allows easy reprocessing without re-ingesting data from the source API.
- Allow partitioning of the data

### Database Indexing
- Indexes were added on frequently queried columns (e.g., `track_id`, `release_date`, etc).
- Improves query performance for both dbt transformations and analytical workloads.

### Orchestration with Airflow
- Airflow enables **multiple pipelines to run sequentially or in parallel**.
- Built-in retries and task-level isolation improve reliability.
- Centralized monitoring simplifies operational visibility.

### Scalable Transformations with dbt
- dbt enables modular, version-controlled SQL transformations.
- Supports testing at scale to ensure data accuracy.
- Encourages best practices such as incremental models and documentation.

---

## 4. Scaling and Extension Strategy

To scale or extend this pipeline, the following enhancements are planned:

- **Increase Ingested Data Volume**  
  Introduce a distributed processing framework (e.g., Spark) for large-scale transformations.

- **dbt Execution via Cosmos**  
  Use Astronomer Cosmos to run dbt models and tests natively within Airflow, providing a seamless and intuitive integration.

- **CI/CD Integration**  
  Add CI/CD pipelines to automate testing, validation, and deployment of Airflow DAGs and dbt models.

These improvements would make the pipeline more scalable, maintainable, and production-ready.

---

## Conclusion

This project demonstrates a production read, scalable data engineering pipeline built with industry-standard tools. By combining Airflow, MinIO, PostgreSQL, and dbt, the pipeline achieves reliability, performance optimization, and analytical flexibility, while remaining fully containerized and easy to operate.
